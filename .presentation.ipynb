{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a14fafaf-7312-4758-9660-ac505c64ce5e",
   "metadata": {},
   "source": [
    "# <center>Classification des Tweets</center>\n",
    "\n",
    "\n",
    "<center><img alt=\"badbuzz\" src=\"images/pn.png\"/></center><br>\n",
    "\n",
    "Nous allons prédire le sentiment de Tweets (« positif » ou « négatif ») en utilisant plusieurs types de modèles. \n",
    "Nous prendrons un échantillon de 20 000 Tweets sur les 1 600 000 disponibles afin de gagner en efficacité sur les temps de traitement avec des performances acceptables pour certains modèles. Il y aura donc 10 000 Tweets positifs et 10 000 Tweets négatifs sélectionnés afin qu’une classe ne soit pas considérée comme « plus importante » lors de la création des modèles.<br> Nous sommes dans une démarche de classification supervisée, par conséquent on séparera les 20 000 tweets 3 parties :<br>\n",
    "•\tJeu d’entrainement : 12800 Tweets : 64%<br> \n",
    "•\tJeu de validation : 3200 Tweets : 16%<br>\n",
    "•\tJeu de test : 4000 Tweets : 20% <br>\n",
    "Chaque jeu contiendra autant de Tweet positifs que négatifs.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76a4041-562b-40fe-afb8-ead16912ac31",
   "metadata": {},
   "source": [
    "## 1.\tModèles sur-mesure simple<br>\n",
    "Nous allons aborder 3 types de modèles : <br>\n",
    "•\teXtreme Gradient Boosting<br>\n",
    "•\tMultinomial Naïve Bayse<br>\n",
    "•\tLogistic Regression<br><br>\n",
    "Ces modèles prennent en compte des valeurs numériques et non du texte « brut ». Par conséquent, chacun des Tweets sera soumis à un plongement de mot (embedding) **« Bag-of-Words »** et **« TF-IDF »** pour observer l’influence de ces transformations sur les modèles de ces méthodes. Aussi, chaque modèle est soumis à la recherche des meilleurs hyperparamètres via GRID-SEARCH CV complétée de l’option StratifiedKFold.<br><br>\n",
    "### **1.1\teXtreme Gradient Boosting**<br>\n",
    "Cet algorithme fonctionne de manière séquentielle. Il va créer un premier modèle qu’il va évaluer. A partir de cette première évaluation, l’algorithme va pondérer les individus en fonction de la performance de leur prédiction et va ensuite recréer un nouveau modèle. Pour notre échantillon, ce modèle génère un AUC=0.7 sur les 2 embeddings cités précédemment.<br><br>\n",
    "### **1.2\tMultinomial Naïve Bayse**<br>\n",
    "Cet algorithme est idéal pour la classification de données textuelles (via une discrétisation de l’effectif des mots). Selon Bayse, le calcul de la probabilité conditionnelle consiste à calculer la probabilité de l’évènement étant donné l’occurrence d’un autre évènement. Cependant, l’algorithme vise à simplifier le calcul en considérant que chaque variable est indépendante des autres et par ce fait, chaque probabilité de chaque variable est calculée indépendamment.   L’aspect multinomial permet d’interpréter la fréquence des mots. Pour notre échantillon, ce modèle génère un AUC=0.73 (embedding TF-IDF) et AUC = 0.72 (embedding Bag-of-Word).<br><br>\n",
    "### **1.3\tLogistic Regression**<br>\n",
    "Ce modèle estime la probabilité qu’un événement ait lieu sur la base d’un ensemble de variables explicatives c’est-à-dire un ensemble d’évènements susceptibles d’influencer cet événement. Le résultat du classement étant une probabilité, si la valeur est supérieure à 0.5, le Tweet est considéré comme positif et si la valeur est inférieure à 0.5, il est considéré comme négatif. Pour notre échantillon, ce modèle génère un AUC=0.72 sur les 2 embeddings cités précédemment.<br><br>\n",
    "### **Conclusion**<br>\n",
    "On remarque que les résultats générés par ces 3 classifieurs sont presque identiques avec le meilleur score proposé par le Multinomial Naïve Bayse et un embedding en TF-IDF : AUC=0.73\n",
    "\n",
    "<center><img alt=\"AUC\" src=\"images/modele/mnb/tf/auc_mnb_tf.png\"/></center><br>\n",
    "<center><img alt=\"AUC\" src=\"images/modele/mnb/tf/class_report_mnb_tf.png\"/></center><br>\n",
    "\n",
    "Voici un résumé des résultats obtenu pour chacun des modèles :<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225e362d-14db-45ee-b066-2a8949cc7706",
   "metadata": {},
   "source": [
    "<table>\n",
    "      <tr>\n",
    "        <th><center>Modèle</center></th>\n",
    "        <th><center>Embedding</center></th>\n",
    "        <th><center>AUC</center></th>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td><center>XGB</center></td>\n",
    "        <td><center>BoW</center></td>\n",
    "        <td><center>0.70</center></td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td><center>XGB</center></td>\n",
    "        <td><center>TF-IDF</center></td>\n",
    "        <td><center>0.70</center></td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td><center>NMF</center></td>\n",
    "        <td><center>BoW</center></td>\n",
    "        <td><center>0.72</center></td>\n",
    "      </tr>\n",
    "        <tr>\n",
    "        <td><center>NMF</center></td>\n",
    "        <td><center>TF-IDF</center></td>\n",
    "        <td><center>0.73</center></td>\n",
    "        </tr>\n",
    "       <tr>\n",
    "        <td><center>LR</center></td>\n",
    "        <td><center>BoW</center></td>\n",
    "        <td><center>0.72</center></td>\n",
    "       </tr>\n",
    "          <tr>\n",
    "        <td><center>LR</center></td>\n",
    "        <td><center>TF-IDF</center></td>\n",
    "        <td><center>0.72</center></td>\n",
    "      </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf180eb-ae43-4cc6-947d-f4c15b776241",
   "metadata": {},
   "source": [
    "## 2.\tModèles sur-mesure avancés\n",
    "Nous allons utiliser des modèles basés sur des réseaux de neurones récurrents (RNN : recurrent neural network) qui sont adaptés à des entrées de tailles variables (ici des Tweets de longueurs différents). En effet ces réseaux contiennent des neurones pour lesquels leur sortie est connectée à l’entrée du neurone afin de considérer les évènements passés. Cependant, les RNNs se heurtent au problème de disparition du gradient pour apprendre à mémoriser des évènements passés. \n",
    "Par conséquent pour parlier à ce problème il a été créé le réseau LSTM (Long Short-Term Memory) qui possède une cellule mémoire interne pilotée par 3 portes :<br> \n",
    "•\tLa porte d'entrée décide si l'entrée doit modifier le contenu de la cellule<br> \n",
    "•\tLa porte d'oubli décide s'il faut remettre à 0 le contenu de la cellule<br> \n",
    "•\tLa porte de sortie décide si le contenu de la cellule doit influer sur la sortie du neurone<br><br> \n",
    "Dans le cas de l’utilisation du LSTM, les sorties de tous les neurones sont réinjectées en entrées de tous les neurones ; de plus les connexions nécessaires au pilotage des cellules mémoire engendrent un réseau très « lourd » comparés aux couches RNN simples. Afin de réduire le nombre de paramètre du réseau, nous allons aussi utiliser une variante du LSTM appelée GRU (Gated Recurrent Unit) qui supprime la porte d’oubli. <br> \n",
    "Dans le cadre de notre étude nous allons donc utiliser et analyser les performances des 3 modèles suivants :<br> \n",
    "•\t**Simple RNN**<br> \n",
    "•\t**LSTM**<br> \n",
    "•\t**GRU**<br><br> \n",
    "Comme pour les modèles sur-mesure simples vus précédemment, nous devons transformer les Tweets en valeurs numériques. Dans un premier temps, nous allons juste utiliser un Tokenizer sur les 3 modèles puis nous allons utiliser indépendamment les 3 types « d'embedding » suivants :<br> \n",
    "•\t**Word2Vec**<br>\n",
    "•\t**GloVe**<br>\n",
    "•\t**FastText**<br>\n",
    "•\t**USE**<br> <br> \n",
    "Ces embedding seront ajoutés aux 3 modèles sous forme de couches pré-entrainées et permettront de prendre en compte différents aspects d’un texte comme la position des mots dans le texte (syntaxe), la signification des mots (sémantique), la prise en compte du sens du mot pouvant en avoir plusieurs en fonction du contexte (polysémie). L’embedding USE se place plutôt sur un niveau de la signification des phrases plutôt que des mots individuellement. Par conséquent, certains embeddings sont plus ou moins consommateur de ressources.<br>\n",
    "Chaque embedding a des méthodes de fonctionnement différents : <br>  \n",
    "•\t**Word2Vec** : crée par Google, sont des réseaux de neurones artificiels à deux couches entraînées pour reconstruire le contexte linguistique des mots. Le modèle utilise 2 architectures pour le calcul des vecteurs : CBOW (prédiction du mot en fonction du contexte) et Skip-gram (prédiction du contexte en fonction du mot)<br> \n",
    "\n",
    "•\t**GloVe** : Global Vectors for word representation : crée par Stanford University, cette méthode de plongement utilise la matrice de cooccurrences qui indique la fréquence à laquelle une paire de mots particulière apparaît ensemble<br> \n",
    "\n",
    "•\t**FastText** : c’est une extension à l’embedding Word2vec ; Word2Vec fourni un plongement de mots alors que FastText fourni un plongement sur n-grams apportant ainsi des informations sur la structure du langage<br> \n",
    "\n",
    "•\t**Universal Sentence Encoder (USE)** : encodage sur la phrase complète au lieu d’encoder mot par mot. On peut ainsi étudier la similarité de sens au niveau de phrase et permettant ainsi une meilleure performance sur les taches de classification en utilisant moins de données d’entrainement supervisés.<br> \n",
    "\n",
    "### Conclusion \n",
    "\n",
    "Dans notre contexte, on remarque que le modèle GRU avec l'embedding GloVe apporte la meilleure performance (AUC = 0.76)<br>\n",
    "<center><img alt=\"AUC\" src=\"images/modele/rnn/gru/glove/rnn_gru_glove_auc.png\"/></center><br>\n",
    "<center><img alt=\"AUC\" src=\"images/modele/rnn/gru/glove/rnn_gru_glove_class_report.png\"/></center><br>\n",
    "\n",
    "\n",
    "Voici les performances enregistrées pour chacun des modèles avec l'embedding associé :<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4c919c-ebd8-409e-a5ac-2d36cdc0a0f2",
   "metadata": {},
   "source": [
    "<table>\n",
    "      <tr>\n",
    "        <th><center>Modèle</center></th>\n",
    "        <th><center>Embedding</center></th>\n",
    "        <th><center>AUC</center></th>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td><center>Simple RNN</center></td>\n",
    "        <td><center>Aucun</center></td>\n",
    "        <td><center>0.62</center></td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td><center>LSTM</center></td>\n",
    "        <td><center>Aucun</center></td>\n",
    "        <td><center>0.62</center></td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td><center>GRU</center></td>\n",
    "        <td><center>Aucun</center></td>\n",
    "        <td><center>0.62</center></td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td><center>Simple RNN</center></td>\n",
    "        <td><center>GloVe</center></td>\n",
    "        <td><center>0.71</center></td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td><center>LSTM</center></td>\n",
    "        <td><center>GloVe</center></td>\n",
    "        <td><center>0.75</center></td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td><center>GRU</center></td>\n",
    "        <td><center>GloVe</center></td>\n",
    "        <td><center>0.76</center></td>\n",
    "      </tr>\n",
    "          <tr>\n",
    "        <td><center>Simple RNN</center></td>\n",
    "        <td><center>Word2Vec</center></td>\n",
    "        <td><center>0.51</center></td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td><center>LSTM</center></td>\n",
    "        <td><center>Word2Vec</center></td>\n",
    "        <td><center>0.59</center></td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td><center>GRU</center></td>\n",
    "        <td><center>Word2Vec</center></td>\n",
    "        <td><center>0.58</center></td>\n",
    "      </tr>\n",
    "          <tr>\n",
    "        <td><center>Simple RNN</center></td>\n",
    "        <td><center>FastText</center></td>\n",
    "        <td><center>0.51</center></td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td><center>LSTM</center></td>\n",
    "        <td><center>FastText</center></td>\n",
    "        <td><center>0.51</center></td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td><center>GRU</center></td>\n",
    "        <td><center>FastText</center></td>\n",
    "        <td><center>0.53</center></td>\n",
    "      </tr>\n",
    "          <tr>\n",
    "        <td><center>Simple RNN</center></td>\n",
    "        <td><center>USE</center></td>\n",
    "        <td><center>0.58</center></td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td><center>LSTM</center></td>\n",
    "        <td><center>USE</center></td>\n",
    "        <td><center>0.65</center></td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td><center>GRU</center></td>\n",
    "        <td><center>USE</center></td>\n",
    "        <td><center>0.64</center></td>\n",
    "      </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24a70d2-eff6-4084-8bcc-36b223659de6",
   "metadata": {},
   "source": [
    "## 3.\tModèle avancé BERT\n",
    "Bidirectional Encoder Representations from Transformers (BERT), développé par Google AI, permet de représenter un mot de façon contextuelle, c’est-à-dire que chaque mot est représenté en fonction de son sens dans le contexte rencontré. Par exemple le « rouge » aura une représentation différente dans « J’achète du rouge pour les invités » et « La voiture sera peinte en rouge ». De plus BERT utilise une technique appelée **Masked LM (MLM)**, qui consiste à masquer aléatoirement des mots dans une phrase et essai de les prédire en utilisant le contexte de la phrase à gauche et à droite du mot masqué (aspect bidirectionnel) et ce de manière simultanée. BERT applique un mécanisme d'attention pour comprendre les relations entre les mots de la phrase ; pour exemple, si l'on prend la phrase \"tu as une souris sur ton ordinateur\", BERT va prêter attention au mot \"ordinateur\" pour déterminer le sens du mot \"souris\".<br>\n",
    "Dans notre contexte, le modèle de BERT nous permet d'obtenir un AUC de 0.88 :<br>\n",
    "<center><img alt=\"AUC\" src=\"images/modele/bert/m1_TFBertForSequenceClassification/88/auc_bert_m1.png\"/></center><br>\n",
    "<center><img alt=\"AUC\" src=\"images/modele/bert/m1_TFBertForSequenceClassification/88/class_report_bert_m1.png\"/></center><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba47073-6c31-4284-9617-0200b1db9fc9",
   "metadata": {},
   "source": [
    "# <center>Démarche MLOps<center>\n",
    "\n",
    "<center><img alt=\"MLOPS\" src=\"images/mlops.png\"/></center><br>\n",
    "\n",
    "\n",
    "Après avoir récupéré les données puis procédé au nettoyage et à l’analyse (EDA : Exploratory Data Analysis), la création de plusieurs types de modèles avec l’utilisation de plusieurs types embeddings a été réalisé pour classer les Tweets (positif ou négatif). Chaque modèle est enregistré sur MLFLOW ainsi que leurs performances associées.<br><br>\n",
    "Ensuite un pipeline de déploiement continu a été réalisé. En effet, le meilleur modèle sélectionné a été déposé sur GitHub et via **GitHub actions** le modèle et l'API FLASK associés vont être automatiquement déposés sur un serveur en ligne (Heroku). Grâce au développement de l’API via **FLASK**, on peut interroger notre modèle (déposé sur le serveur en ligne) en lui envoyant un Tweet ; on obtiendra ainsi une réponse « positif » ou « négatif » afin de connaitre la classification du Tweet envoyé. A chaque nouveau jeu de données reçu, un nouveau modèle est créé, des tests unitaires sont lancés pour vérifier si tout s’est bien passé ; si le dernier modèle crée est plus performant que le précédent, le processus cité dans le dernier paragraphe est renouvelé. <br>\n",
    "Voici plus précisément le pipeline déployé pour ce projet : <br>\n",
    "<center><img alt=\"pipeline\" src=\"images/pipeline.png\"/></center><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d14184-4e9c-45d8-8693-348936ce536f",
   "metadata": {},
   "source": [
    "# FIN"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
